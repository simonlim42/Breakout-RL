{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from atari_wrappers import NoopResetEnv\n",
    "from atari_wrappers import MaxAndSkipEnv\n",
    "from atari_wrappers import EpisodicLifeEnv\n",
    "from atari_wrappers import FireResetEnv\n",
    "from atari_wrappers import WarpFrame\n",
    "from atari_wrappers import ScaledFloatFrame\n",
    "from atari_wrappers import ClipRewardEnv\n",
    "from atari_wrappers import FrameStack\n",
    "\n",
    "from model import DQN\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "import os\n",
    "import imageio\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import PIL.ImageDraw as ImageDraw\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def makeAtari(env_id, render_mode=None,max_episode_steps=400000):\n",
    "    if render_mode == None:\n",
    "        env = gym.make(env_id,render_mode='rgb_array')\n",
    "    elif render_mode == 'human':\n",
    "        env = gym.make(env_id,render_mode='human')\n",
    "    env._max_episode_steps = max_episode_steps\n",
    "\n",
    "    env = NoopResetEnv(env, noop_max=1)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    return env\n",
    "\n",
    "\n",
    "def wrapDeepmind(env):\n",
    "    env = EpisodicLifeEnv(env)\n",
    "    env = ClipRewardEnv(env)\n",
    "    env = WarpFrame(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "#Take rgb array and greyscale it \n",
    "def frameProcessor(n_frame):\n",
    "    n_frame = torch.from_numpy(n_frame)\n",
    "    h = n_frame.shape[-2]\n",
    "    return n_frame.view(1,h,h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environmnet setup \n",
    "When setting up the environemnt there are a few things we need to define\n",
    "* The device, if we are using the CPU or the GPU\n",
    "* The environemtn id, as we are using the gym library we need to make sure the environemnt id matches what is written in gym\n",
    "\n",
    "Using the frame processor function from above we also get the chanell height and width which is later used to define the shape of our input for our states \n",
    "\n",
    "We alos define the policy and target net. Applying initial wieghts (zero values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use GPU if it is available if not use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(device)\n",
    "\n",
    "#our atari environemnt and the verision we are using\n",
    "envId = 'Breakout-v4'\n",
    "env = makeAtari(envId)\n",
    "env = wrapDeepmind(env)\n",
    "\n",
    "c,h,w = frameProcessor(env.reset()).shape\n",
    "actions = env.action_space.n\n",
    "\n",
    "policyNet = DQN(actions, device).to(device)\n",
    "policyNet.apply(policyNet.init_weights)\n",
    "\n",
    "targetNet = DQN(actions, device).to(device)\n",
    "targetNet.load_state_dict(policyNet.state_dict())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "The hyperparameters difine our decay rate, memory size etc. They are tweaked to fined optimum values for our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 1000000\n",
    "TARGET_UPDATE = 10000\n",
    "NUM_STEPS = 20000000\n",
    "M_SIZE = 10000\n",
    "POLICY_UPDATE = 4\n",
    "EVALUATE_FREQ = 200000\n",
    "NUM_EPISODE = 100000\n",
    "optimizer = optim.Adam(policyNet.parameters(), lr=0.0000625, eps=1.5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer\n",
    "The replay buffer is where all our states,actions,rewards,done(if the episode is ended or not) and priority. The replay buffer is later used in evaluation and optimize methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#REPLAY BUFFER \n",
    "statesM = torch.zeros((M_SIZE, 5, h, w), dtype=torch.uint8) #state is a tensor of zeros with length capacity and in the shae c,h,w with type uint8\n",
    "actionsM = torch.zeros((M_SIZE, 1), dtype=torch.long) #action is a tensor of zeros with length capacity with only 1 item in its shape with type long  \n",
    "rewardsM = torch.zeros((M_SIZE, 1), dtype=torch.int8) #rewardsM is a tensor of zeros with length capacity with only 1 item in its shape with type unit8\n",
    "\n",
    "doneM = torch.zeros((M_SIZE, 1), dtype=torch.bool) #doneM< is a tensor of zeros with length capacity with only 1 item in its shape with type bool\n",
    "prioritiesM = np.zeros(M_SIZE) #priorities is a numpy array with length capacity, it stores floats that define the proactionBatchbilities for the corresponding index\n",
    "\n",
    "#initial size and position is both zero\n",
    "position = 0\n",
    "\n",
    "def push(state, action, reward, done, priority,position):\n",
    "    statesM[position] = state \n",
    "    actionsM[position,0] = action\n",
    "    rewardsM[position,0] = reward\n",
    "    doneM[position,0] = done\n",
    "    prioritiesM[position] = priority\n",
    "\n",
    "    #iterate position till it reaches Memory size\n",
    "    #if capacity is reached then restart buffer at zero\n",
    "    if position < M_SIZE:\n",
    "        position +=1\n",
    "    else:\n",
    "        position = 0\n",
    "\n",
    "\n",
    "def getPriorities(actionBatchtchSize):\n",
    "        #Temporary store for priorities\n",
    "        arr = prioritiesM\n",
    "\n",
    "        # Create a boolean array indicating which elements are not zero\n",
    "        nonZeroMask = arr != 0\n",
    "\n",
    "        # Use the boolean array to get a subset of the original array\n",
    "        \n",
    "        tempPrio = arr[nonZeroMask]\n",
    "        prioritySum = np.sum(tempPrio)\n",
    "        samplingProactionBatchbility = tempPrio / prioritySum\n",
    "        indexValues = []\n",
    "\n",
    "        #Sample random integers given proactionBatchbility\n",
    "        for i in range(actionBatchtchSize):\n",
    "            indexValues.append(np.random.choice(len(samplingProactionBatchbility), p=samplingProactionBatchbility))\n",
    "        return indexValues\n",
    "\n",
    "def sample(actionBatchtchSize):\n",
    "    #get index values actionBatchsed on their priorities\n",
    "    i = getPriorities(actionBatchtchSize)\n",
    "    \n",
    "    frameStack = statesM[i, :4] #for each index in i get its frame stack\n",
    "    stateBatch = frameStack[1]  #the state is the current state\n",
    "    actionBatch = actionsM[i].to(device) #get the action at the index, convert it to the device type\n",
    "    rewardBatch = rewardsM[i].to(device).float() #get the reward at the index, convert it to the device type and then to a float\n",
    "    doneBatch = doneM<[i].to(device).int()  #get the reward at the index, convert it to the device type and then to a int \n",
    "    return frameStack, actionBatch, rewardBatch, stateBatch, doneBatch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below is only used for the video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _label_with_episode_number(frame, episode_num):\n",
    "    im = Image.fromarray(frame)\n",
    "\n",
    "    drawer = ImageDraw.Draw(im)\n",
    "\n",
    "    if np.mean(im) < 128:\n",
    "        text_color = (255,255,255)\n",
    "    else:\n",
    "        text_color = (0,0,0)\n",
    "    drawer.text((im.size[0]/20,im.size[1]/12), f'Num steps: {episode_num+1}', fill=text_color)\n",
    "\n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action selector\n",
    "Depending on the current epsilon value we sample a random value and depnding on if it is greater or less than the epsilon we either pick the best action or a random value.\n",
    "\n",
    "There is another function in this cell ***optimumAction*** this is used only in evaluation. It has an epsilon of 0.05 so 95% of the time we are picking the optimum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsCounter = EPS_START\n",
    "epsDecay = (EPS_START - EPS_END)/EPS_DECAY\n",
    "\n",
    "def selectAction(epsCounter, state, training=False):\n",
    "        sample = random.random() #Sample a random value\n",
    "        # if trainning is not false\n",
    "        if training:\n",
    "            #Decay epsilon by decay factor\n",
    "            epsCounter -= epsDecay\n",
    "            #Chose max between the current epsilon and the final epsilon. \n",
    "            #if current eps falls bellow 0.1 then it will pick 0.1\n",
    "            epsCounter = max(epsCounter, EPS_END)\n",
    "\n",
    "        #If the sample is greater than current eps    \n",
    "        if sample > epsCounter:\n",
    "            with torch.no_grad():\n",
    "                #using the policy net pick the action with the highest state action value\n",
    "                max_index = policyNet(state).argmax(dim=1)\n",
    "                a = max_index.cpu().view(1,1)\n",
    "        else:\n",
    "            #If sample is not greater than eps then randomly chose an action \n",
    "            a = torch.tensor([[random.randrange(4)]], device=device, dtype=torch.long)\n",
    "        #Return action as a numpy array and the current eps\n",
    "        return a.numpy()[0,0].item(), epsCounter\n",
    "\n",
    "def optimumAction(state):\n",
    "    sample = random.random() #Sample a random value\n",
    "    eps = 0.05\n",
    "\n",
    "    #If the sample is greater than current eps    \n",
    "    if sample > eps:\n",
    "        with torch.no_grad():\n",
    "            #using the policy net pick the action with the highest state action value\n",
    "            maxIndex = policyNet(state).argmax(dim=1)\n",
    "            a = maxIndex.cpu().view(1,1)\n",
    "    else:\n",
    "        #If sample is not greater than eps then randomly chose an action \n",
    "        a = torch.tensor([[random.randrange(4)]], device=device, dtype=torch.long)\n",
    "    #Return action as a numpy array and the current eps\n",
    "    return a.numpy()[0,0].item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Double DQN\n",
    "def optimize_model(train):\n",
    "    if not train:#\n",
    "        return\n",
    "    state_batch, action_batch, reward_batch, n_state_batch, done_batch = sample(BATCH_SIZE)\n",
    "\n",
    "    q = policyNet(state_batch).gather(1, action_batch)\n",
    "    q_next_values=policyNet(n_state_batch).gather(1 , action_batch)\n",
    "    a_prime = policyNet(n_state_batch).max(1)[1]\n",
    "    q_target_next_values = targetNet(n_state_batch).detach()\n",
    "    q_target_sa_prime = q_target_next_values.gather(1, a_prime.unsqueeze(1))\n",
    "    q_target_sa_prime = q_target_sa_prime.squeeze()\n",
    "\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (q_target_sa_prime * GAMMA)*(1.-done_batch[:,0]) + reward_batch[:,0]\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(q, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policyNet.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate loss/priority\n",
    "to calculate the loss we calculate the difference between the predicted reward and the actual reward multiplied by the discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPriority(state,reward,done):\n",
    "    tState = torch.tensor(state)\n",
    "\n",
    "    qV = policyNet(tState).max(1)[0].cpu().detach() \n",
    "    nq = targetNet(tState).max(1)[0].cpu().detach()\n",
    "    \n",
    "    doneBatch = torch.tensor(np.array([int(done)]))\n",
    "\n",
    "    expected_state_action_values = (nq * GAMMA)*(1.-doneBatch) + reward\n",
    "\n",
    "    loss = F.smooth_l1_loss(qV, expected_state_action_values)\n",
    "\n",
    "    return abs(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation \n",
    "The local network is evaluated every n steps (based on the hyperparams). It runs through 5 episodes using the ***OptimumAction*** selector. This makes sure we are using the best actions. We sum up the rewards and this is printed into a text file along with the number of steps we have done and the current eps. We also have a counter for the ***maxVal*** whhch is the maximum reward gained during the current evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "frameStore=[]\n",
    "filenames = []\n",
    "\n",
    "def evaluate(step, policy_net, device, env, n_actions, curr_eps,train):\n",
    "    \n",
    "    #Numver of episodes to average over\n",
    "    numEpisode=5\n",
    "    \n",
    "    eRewards = []\n",
    "    eQ=[]\n",
    "    q = deque(maxlen=5)\n",
    "    countingQ=0\n",
    "    countingSteps=0\n",
    "    maxval=0\n",
    "\n",
    "    for i in range(numEpisode):\n",
    "        env.reset()\n",
    "        eReward = 0\n",
    "        for _ in range(15): # no-op\n",
    "            frame, _, done, _ = env.step(0)\n",
    "            frame = frameProcessor(frame)\n",
    "            q.append(frame)\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            state = torch.cat(list(q))[1:].unsqueeze(0)\n",
    "            #Get optimum action\n",
    "            action = optimumAction(state)\n",
    "            frame, reward, done, info = env.step(action)\n",
    "            frame = frameProcessor(frame)\n",
    "            q.append(frame)\n",
    "               \n",
    "            eReward += reward\n",
    "            countingQ+=max(policy_net(state)[0])\n",
    "            countingSteps+=1\n",
    "        maxval=max(maxval,eReward)\n",
    "        eQ.append(countingQ/countingSteps)\n",
    "        eRewards.append(eReward)\n",
    "\n",
    "    f = open(\"file.txt\",'a') \n",
    "    f.write(\"Average reward: %f, Steps: %d, number of eps: %d, current eps: %f, Maxval: %f Average predicted Q: %f \\n\" % (float(sum(eRewards))/float(numEpisode), step, numEpisode,float(curr_eps),maxval,(float(sum(eQ)))/(float(numEpisode))))\n",
    "    f.close()\n",
    "    for i in range(2):\n",
    "        env.reset()\n",
    "        for _ in range(10): # no-op\n",
    "            frame, _, done, _ = env.step(0)\n",
    "            frame = frameProcessor(frame)\n",
    "            q.append(frame)\n",
    "        while not done:\n",
    "            frameTest = env.render(\"rgb_array\")\n",
    "            state = torch.cat(list(q))[1:].unsqueeze(0)\n",
    "            action = optimumAction(state)\n",
    "            frame, reward, done, info = env.step(action)\n",
    "            frame = frameProcessor(frame)\n",
    "            q.append(frame)\n",
    "            frameStore.append(_label_with_episode_number(frameTest, episode_num=step))\n",
    "            \n",
    "    imageio.mimwrite(os.path.join('./videos/', 'random_agent.gif'), frameStore, frameProcessors=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main section (trainning)\n",
    "In this main section we train our agent. Initially we do 15 steps of a no-op. This makes sure our starting state is random. we append this frames to the queue. After having our initial state we then check if the length of our replay buffer is greater than 5000. This is just to make sure we have enough data to train our agent on. We then select an action based on the current epsilon. Storing the State,reward,action,done in the replay buffer.\n",
    "\n",
    "This then loops for the number of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000000 [00:00<?, ?it/s]/var/folders/2z/0p788lv54yddrxzp4bhbsyj80000gn/T/ipykernel_6726/326040526.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tState = torch.tensor(state)\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:280: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n",
      "  0%|          | 0/20000000 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "The directory '/Users/blank/Desktop/Reinforcement Learning/Breakout FINAL/videos' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/blank/Desktop/Reinforcement Learning/Breakout FINAL/Double_DQN_Breakout.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/blank/Desktop/Reinforcement%20Learning/Breakout%20FINAL/Double_DQN_Breakout.ipynb#ch0000012?line=36'>37</a>\u001b[0m     targetNet\u001b[39m.\u001b[39mload_state_dict(policyNet\u001b[39m.\u001b[39mstate_dict())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/blank/Desktop/Reinforcement%20Learning/Breakout%20FINAL/Double_DQN_Breakout.ipynb#ch0000012?line=38'>39</a>\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m EVALUATE_FREQ \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/blank/Desktop/Reinforcement%20Learning/Breakout%20FINAL/Double_DQN_Breakout.ipynb#ch0000012?line=39'>40</a>\u001b[0m     evaluate(step, policyNet, device, env, actions, cuur_eps, train)\n",
      "\u001b[1;32m/Users/blank/Desktop/Reinforcement Learning/Breakout FINAL/Double_DQN_Breakout.ipynb Cell 12'\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(step, policy_net, device, env, n_actions, curr_eps, train)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/blank/Desktop/Reinforcement%20Learning/Breakout%20FINAL/Double_DQN_Breakout.ipynb#ch0000009?line=71'>72</a>\u001b[0m         q\u001b[39m.\u001b[39mappend(frame)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/blank/Desktop/Reinforcement%20Learning/Breakout%20FINAL/Double_DQN_Breakout.ipynb#ch0000009?line=72'>73</a>\u001b[0m         frameStore\u001b[39m.\u001b[39mappend(_label_with_episode_number(frameTest, episode_num\u001b[39m=\u001b[39mstep))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/blank/Desktop/Reinforcement%20Learning/Breakout%20FINAL/Double_DQN_Breakout.ipynb#ch0000009?line=74'>75</a>\u001b[0m imageio\u001b[39m.\u001b[39;49mmimwrite(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39m'\u001b[39;49m\u001b[39m./videos/\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrandom_agent.gif\u001b[39;49m\u001b[39m'\u001b[39;49m), frameStore, frameProcessors\u001b[39m=\u001b[39;49m\u001b[39m60\u001b[39;49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/imageio/v2.py:361\u001b[0m, in \u001b[0;36mmimwrite\u001b[0;34m(uri, ims, format, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m imopen_args \u001b[39m=\u001b[39m decypher_format_arg(\u001b[39mformat\u001b[39m)\n\u001b[1;32m    360\u001b[0m imopen_args[\u001b[39m\"\u001b[39m\u001b[39mlegacy_mode\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m \u001b[39mwith\u001b[39;00m imopen(uri, \u001b[39m\"\u001b[39;49m\u001b[39mwI\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mimopen_args) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m    362\u001b[0m     \u001b[39mreturn\u001b[39;00m file\u001b[39m.\u001b[39mwrite(ims, is_batch\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/imageio/core/imopen.py:113\u001b[0m, in \u001b[0;36mimopen\u001b[0;34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m     request\u001b[39m.\u001b[39mformat_hint \u001b[39m=\u001b[39m format_hint\n\u001b[1;32m    112\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     request \u001b[39m=\u001b[39m Request(uri, io_mode, format_hint\u001b[39m=\u001b[39;49mformat_hint, extension\u001b[39m=\u001b[39;49mextension)\n\u001b[1;32m    115\u001b[0m source \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m<bytes>\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(uri, \u001b[39mbytes\u001b[39m) \u001b[39melse\u001b[39;00m uri\n\u001b[1;32m    117\u001b[0m \u001b[39m# fast-path based on plugin\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39m# (except in legacy mode)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/imageio/core/request.py:248\u001b[0m, in \u001b[0;36mRequest.__init__\u001b[0;34m(self, uri, mode, extension, format_hint, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid Request.Mode: \u001b[39m\u001b[39m{\u001b[39;00mmode\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    247\u001b[0m \u001b[39m# Parse what was given\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse_uri(uri)\n\u001b[1;32m    250\u001b[0m \u001b[39m# Set extension\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[39mif\u001b[39;00m extension \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/imageio/core/request.py:413\u001b[0m, in \u001b[0;36mRequest._parse_uri\u001b[0;34m(self, uri)\u001b[0m\n\u001b[1;32m    411\u001b[0m dn \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(fn)\n\u001b[1;32m    412\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(dn):\n\u001b[0;32m--> 413\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe directory \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m does not exist\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m dn)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: The directory '/Users/blank/Desktop/Reinforcement Learning/Breakout FINAL/videos' does not exist"
     ]
    }
   ],
   "source": [
    "steps_done = 0\n",
    "q = deque(maxlen=5)\n",
    "done = True\n",
    "eps = 0\n",
    "episode_len = 0\n",
    "\n",
    "\n",
    "for step in tqdm(range(NUM_STEPS)):\n",
    "    if done: # life reset !!!\n",
    "        env.reset()\n",
    "        sum_reward = 0\n",
    "        episode_len = 0\n",
    "        img, _, _, _ = env.step(1) # BREAKOUT specific !!!\n",
    "        for i in range(15): # no-op\n",
    "            frame, _, _, _ = env.step(0)\n",
    "            frame = frameProcessor(frame)\n",
    "            q.append(frame)\n",
    "        \n",
    "    train = len(prioritiesM) > 50000\n",
    "    # Select and perform an action\n",
    "    state = torch.cat(list(q))[1:].unsqueeze(0)\n",
    "    action, epsCounter = selectAction(epsCounter,state,train)\n",
    "\n",
    "    #get value from the environemnt based on the action taken\n",
    "    frame, reward, done, info = env.step(action)\n",
    "    #Process to image to grey scale the image\n",
    "    frame = frameProcessor(frame)\n",
    "\n",
    "    #Get priority\n",
    "    priority = getPriority(state,reward,done)\n",
    "\n",
    "    # 5 frame as memory\n",
    "    q.append(frame)\n",
    "    push(torch.cat(list(q)).unsqueeze(0), action, reward, done,priority,position) # here the frame means next frame from the previous time step\n",
    "    episode_len += 1\n",
    "\n",
    "    # Perform one step of the optimization (on the target network)\n",
    "    if step % POLICY_UPDATE == 0:\n",
    "        (train)\n",
    "\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if step % TARGET_UPDATE == 0:\n",
    "        targetNet.load_state_dict(policyNet.state_dict())\n",
    "    \n",
    "    if step % EVALUATE_FREQ == 0:\n",
    "        evaluate(step, policyNet, device, env, actions, cuur_eps, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
